% 11-761 Language and Statistics
% Assignment 3 

\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[left=1in,top=1in,right=1in,bottom=1in,nohead,nofoot]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{float}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{todonotes}
\usepackage{wrapfig}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\ngram}{\mbox{$n$-gram }}
\newcommand{\ngrams}{\mbox{$n$-grams }}

\newcommand{\leocomment}[1]{\todo[color=red!40,caption={Leo's comment}]{#1}} 
\newcommand{\hectcomment}[1]{\todo[color=green!40,caption={Hector's comment}]{#1}} 
\newcommand{\dicomment}[1]{\todo[color=violet!40,caption={Di's comment}]{#1}} 
\newcommand{\zhoucomment}[1]{\todo[color=blue!40,caption={Zhou's comment}]{#1}} 

\restylefloat{table}
\restylefloat{figure}

\begin{document}

\title{11-761 Language and Statistics - Spring 2013\\
Course Project}
\author{Leonid (Leo) Boytsov, Zhou Yu, Di Wang, Hector Liu}
\date{}
\maketitle

\begin{abstract}
We took a machine learning approach and train two discriminative classifiers: 
a soft-margin SVM and a regularized logistic regression.
The SVM is used for classification, while the logistic regression is used to produce posterior probabilities.
To reduce the risk of overfitting, we generate additional training data and testing data.
\end{abstract}

%\thispagestyle{empty}
%\setlength{\parindent}{0pt}

\section{Introduction}
The project tasks includes classification and assignment of posterior probabilities to documents.
Our group decided to focus on the classification task.
It is commonly believed that discriminative classifiers are superior to generative ones,
in particular, because generative classifiers require more training data \leocomment{Zhou, do you know any other good references for this? A nice short explanation on data-sparsity and necessity to smooth
may also be good here.}\cite{roni2013,jordan2002discriminative,bishop2007generative}.
\footnote{See also \url{http://xiaodong-yu.blogspot.com/2009/10/good-summary-on-generative-vs.html}.}
Therefore, we decided to generate linguistically-motivated features and to employ a discriminative classifier.
To reduce the risk of overfitting, we generate additional training data and testing data.
The details are given in subsequent sections. The individual contributions of team members are summarized in Table\ref{TableContrib}.


\leocomment{Please, revise.}
\begin{table}[H]
\begin{tabular}{p{1in}|p{5in}}
\hline
Leo Boytsov &  Wrote code to produce features based on \ngram models (including models based on POS-tag sequence), generated additional training/testing data, and fine-tuned machine learning models;
\\\hline
Zhou You & Implemented and designed code for classification and computation of posterior probabilities;
\\\hline
Di Wang & Tested additional features, in particular, based on word co-occurrences and
carried out error analysis;
\\\hline
Hector Liu & Helped Leo to generate POS-tag based \ngram models, wrote a 
reliable RunMe script that feeds language-model features to machine learning code
and extracts results.
\\\hline
Everybody & Participated in the design process and described their his/her work
in the report and the presentation.
\\\hline
\end{tabular}

\caption{\label{TableContrib}Individual contributions of team members.}
\end{table}

\section{Training the Models} 
\subsection{Choosing Features} Because fake text were generated using a tri-gram model, we expected that
higher order \ngram models ($n>3$) should have different perplexities on fake and real data. 
Thus, perplexity values can be used as document features. 

The \ngram models were created with the help of the CMU-Cambridge toolkit\footnote{\url{http://svr-www.eng.cam.ac.uk/~prc14/toolkit.html}}.  The Broadcast News corpus was used for training. 
We employed seven \ngram models for $2 \le n \le 7$, which were smoothed using the Good-Touring method.
For $n>2$ the models included only \ngrams that occurred more than once (the higher is model order the higher is the cutoff value).

In addition, we tried several to compute features using other approaches, 
First, we built \ngram models for sequences of POS tags. To this end, we applied the Stanford POS tagger 
to the Broadcast News corpus. Second, we generated language models based on word co-occurrences. 
\leocomment{Di, please, expand here.}
Finally, we tried to use the total number of words in a document and the number of words \texttt{<UNK>}.
All these additional features were inferior to perplexities computed from the \ngram models.
In particular, the accuracy of classifier built on top of the POS-tag langauge models,
was less than 70\%. Combining POS-tag-based \ngram models with regular \ngram models did not lead
to better classification accuracy.

\subsection{Generating Additional Training/Testing Data}\label{SectGen}
Because the CMU-Cambridge toolkit cannot generate data from a tri-gram model,
we tried to employ the NLTK tookit (using Broadcast News corpus as a training set). 
Unigrams not included in the tir-gram model (provided by instructors), we replaced by the word \texttt{<UNK>}.
Real sentences were randomly sampled from the Broadcast News corpus.
For both real and fake data, document lengths were chosen randomly to mimic distribution in the training data.
Unfortunately, this data proved to be completely useless. The feature values for the generated data
and the training set provided by instructors were substantially different.

Then, we found that the CMU Sphinx toolkit\footnote{\url{http://cmusphinx.sourceforge.net/}} does include a version
of the CMU-Cambridge toolkit that \textbf{can} generate from a tri-gram language model.
However, we could not make the Sphinx toolkit read the provided binary language model.\footnote{It complained that the model was corrupt.} Nor did the Sphinx toolkit 
support generation from the text version of the language model (the \texttt{arpa}-file).
Hence, we back-ported the generating function to the CMU-Cambridge toolkit.

We also took a different approach to generating real data. 
It appears to us that the Broadcast News corpus contains complete documents, but their boundaries are not marked. 
To generate a document containing a desired number of 
words, we, thus, start from the beginning of a random sentence and select a contiguous sequence
of words. Thus, generated documents contain long pieces of coherent text (with similar statistical properties).\leocomment{How does ``similar statistical properties'' sound?}
For the reasons explained int the section \ref{SectTrain},\leocomment{I don't like forward references,
but otherwise, it is hard to explain.} 
on average, a generated document
\textbf{2000 words longer} than  a document from the provided training set.

\subsection{Choosing and Training the Models}\label{SectTrain}
\leocomment{Zhou, please, review and expand if necessary.}
For the purpose of classification we use a soft-margin SVM.
To produce posterior probability we train another discriminative model: a
regularized logistic regression. The soft metric
cannot be computed, if any probability is zero,
hence, posterior probabilities were smoothed.
Additionally, also tested a continuous Naive Bayes (with normal kernel), which is a generative model.
Yet, the Naive Bayes performed badly.
Two regularization methods were tested: the LASSO and the Tikhonov regularization.
The soft-margin,  the smoothing and regularization parameters were chosen
empirically (performance evaluated through 10-fold cross-validation).

The logarithm of observed perplexity value 
is an average perplexity value taken over log-perplexities of document \ngrams.
We adopt a point of view that the log-perplexity is sampled from a random variable,
which is parameterized by a latent parameter representing document topic.
Thus, the average document log-perplexity is a MLE estimate of this random variable mean.
We argue that performance of machine learning models depend on how
well they learn parameters of these distribution, in particular their mean values.

The shorter is a document, the higher is the variance of the mean estimates
and, consequently, the harder it is for the model to ``guess'' true mean values.
Hence, if a document length (or a number of words)
is not included as a feature,
models should produce better results when they are trained on longer documents.
In particular, when we train and test on the provided dev set, we get much worse
accuracy, then when we train on the training set and test on the dev set.
This is somewhat paradoxical, because the dev set contains mostly short documents (around 100-200)
words, while the training set contains few or no documents shorter than 100 words
(i.e., we may view this training set as being not representative).

\begin{table}
\caption{\label{TablePerm}  of the SVM classifier (hard metric) for various data and feature sets.}
\begin{tabular}{c|c|c|c|c|c|c|c}\hline
\multicolumn{4}{c}{Original DEV set} \\\hline\hline
\multicolumn{2}{c|}{Original training set} & \multicolumn{2}{c}{Generated training set} \\\hline
short & complete & short & complete  \\
features & features & features & features \\\hline
0.88  & 0.90 & 0.92 &  0.895  \\\hline
\multicolumn{4}{c}{Generated DEV set} \\\hline\hline
\multicolumn{2}{c|}{Original training set} & \multicolumn{2}{c}{Generated training set} \\\hline
short & complete & short & complete  \\
features & features & features & features \\\hline
0.927 & 0.923   & 0.927 & 0.901  \\\hline
\end{tabular}
\end{table}

\section{Conclusions and Experimental Highlights}
In addition, to the training and dev sets provided by instructors,
we used data generated by our team (see Section \ref{SectGen}).
Our synthetic training and development sets include 10,000 and 2,000 documents, respectively. 


Our experiments indicate that 
an \ngram based approach to detection of synthetic documents (generated
from a 3-gram model) outperforms other methods to generate features.
In particular, building \ngram models over sequences of POS tags was not helpful.

\begin{wrapfigure}{r}{0.5\textwidth}\centering
\includegraphics[scale=0.6]{erroranalysisGram34.png}
\caption{Perplexities for 3- and 4-gram models computed for the \textbf{training} set.\label{Fig1}}
\end{wrapfigure}

We employed two sets of features. The complete set includes perplexities for six \ngram models,
but the short set employs only the 3-gram and the 4-gram models.
When, we use only the provided training data, it does not hurt to include all features into the SVM 
model (see Table~\ref{TablePerf}). As we get more training data, it may be better to use only 
the 3-gram and the 4-gram models. 

As we can see from Figure~\ref{Fig1},
these two models alone separate the fake and real documents almost perfectly.
As we explained in Section \ref{SectGen}, this is due to the fact that the training set
contains only few short documents and the variance.
For a development set, the ``clouds'' of real and fake documents have a considerable overlap.\leocomment{Di, did I get this correctly on the overlap for the DEV data?}

We also believe that there exist a \textbf{latent topic variable} that determines the complexity
of the document lexicon.
If the document contains a lot of rare words, both the 3-gram and the 4-gram models
would be surprised to see it. In contrast, documents with a lot of common words
have low perplexity values for all \ngram models.
The value of document perplexity with respect to any \ngram model varies considerably
and none of the model is sufficient to detect fake sentences.
Yet, this is possible with two models.

Unlike the classification task, it is beneficial to include all features 


\cite{jordan2002discriminative}.

\bibliographystyle{plain}
\bibliography{report}

\end{document}
